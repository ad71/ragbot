{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyUhCvArh3Bw3ilDM3wy9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad71/ragbot/blob/master/multi_agent_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-agent collaboration\n",
        "Multiple agents working on the same state of messages\n",
        "\n",
        "- Can be shared state\n",
        "- Or independent / siloed\n",
        "\n",
        "Then share results"
      ],
      "metadata": {
        "id": "1YhLlwg2gq_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM7r7YLSedyC",
        "outputId": "d3fc3b38-257b-45e6-b59e-a0e84a3d2660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m163.8/199.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_openai langchain_community langchain_experimental langsmith pandas matplotlib langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# optional, add tracing in LangSmith\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'Multi-Agent Collaboration'"
      ],
      "metadata": {
        "id": "-z35_7k0hbQ6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import operator\n",
        "import functools\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    ChatMessage,\n",
        "    HumanMessage,\n",
        "    FunctionMessage\n",
        ")\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "2Oyons5-iDwY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(llm, tools, system_message: str):\n",
        "    '''Create an agent'''\n",
        "    functions = [convert_to_openai_function(t) for t in tools]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                'system',\n",
        "                'You are a helpful AI assistant, collaborating with other assistants. '\n",
        "                'Use the provided tools to progress towards answering the question. '\n",
        "                'If you are unable to fully answer, that\\'s okay, another assistant with different tools '\n",
        "                'will help you where you left off. Execute what you can do to make progress. '\n",
        "                'If you or any of the other assistants have the final answer or deliverable, '\n",
        "                'prefix your response with FINAL ANSWER so the team knows to stop. '\n",
        "                'You have access to the following tools: {tool_names}.\\n{system_message}'\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name='messages')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    prompt = prompt.partial(tool_names=', '.join([tool.name for tool in tools]))\n",
        "    return prompt | llm.bind_functions(functions)"
      ],
      "metadata": {
        "id": "8qjop9uUjLD3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily = TavilySearchResults(max_results=5)\n",
        "repl = PythonREPL() # executes code locally, which can be unsafe when not sandboxed\n",
        "\n",
        "@tool\n",
        "def python_repl(code: Annotated[str, 'The python code to execute to generate your chart']):\n",
        "    '''Use this to execute python code. If you want to see the output of a value, you should print it out with print(...). This is visible to the user'''\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f'Failed to execute. Error: {repr(e)}'\n",
        "    return f'Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}'"
      ],
      "metadata": {
        "id": "keCxrTljlgBB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    sender: str"
      ],
      "metadata": {
        "id": "MLfeeWfMs5K4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "\n",
        "    if isinstance(result, FunctionMessage):\n",
        "        # if its a function message, pass it as it is\n",
        "        pass\n",
        "    else:\n",
        "        # if its an AI message, make the next node think it's a human message to be acted upon\n",
        "        result = HumanMessage(**result.dict(exclude={'type', 'name'}), name=name)\n",
        "\n",
        "    return {\n",
        "        'messages': [result],\n",
        "        'sender': name\n",
        "    }"
      ],
      "metadata": {
        "id": "x92TMglsuBhd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model='gpt-4-1106-preview')"
      ],
      "metadata": {
        "id": "CemqpjW9vUJi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# research agent and node\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [tavily],\n",
        "    system_message='You should provide accurate data for the chart generator to use'\n",
        ")\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name='Researcher')\n",
        "\n",
        "# chart generator\n",
        "chart_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl],\n",
        "    system_message='Any charts you display will be visible by the user'\n",
        ")\n",
        "chart_node = functools.partial(agent_node, agent=chart_agent, name='Chart Generator')"
      ],
      "metadata": {
        "id": "cmKHI3TXvcBK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tool node\n",
        "tools = [tavily, python_repl]\n",
        "tool_executor = ToolExecutor(tools)\n",
        "\n",
        "def tool_node(state):\n",
        "    '''This runs tools in the graph\n",
        "    It takes in an agent action and calls that tool and returns the result\n",
        "    '''\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    tool_input = json.loads(last_message.additional_kwargs['function_call']['arguments'])\n",
        "\n",
        "    if len(tool_input) == 1 and '__arg1' in tool_input:\n",
        "        tool_input = next(iter(tool_input.values()))\n",
        "    tool_name = last_message.additional_kwargs['function_call']['name']\n",
        "    action = ToolInvocation(\n",
        "        tool=tool_name,\n",
        "        tool_input=tool_input\n",
        "    )\n",
        "    response = tool_executor.invoke(action)\n",
        "    function_message = FunctionMessage(content=f'{tool_name} response: {str(response)}', name=action.tool)\n",
        "    return {'messages': [function_message]}"
      ],
      "metadata": {
        "id": "CZYxqt5fwPwR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VDRWUMP2haB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}