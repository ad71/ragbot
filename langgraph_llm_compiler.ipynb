{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMCompiler\n",
    "LLMCompiler is an agent architecture to speed up the execution of agentic tasks by eagerly-executed tasks within a DAG. It also saves costs on redundant token usage by reducing the number of calls to the LLM.\n",
    "<br>\n",
    "It has 3 main components:\n",
    "1. Planner: stream a DAG of tasks\n",
    "2. Task Fetching Unit: Schedules and executes the tasks as soon as they are executable\n",
    "3. Joiner: Responds to the user or triggers a second plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import asyncio\n",
    "import platform\n",
    "import requests\n",
    "import operator\n",
    "import itertools\n",
    "import playwright\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Literal\n",
    "from typing import Iterable\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import MessageGraph\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.function import FunctionMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import chain as chain_decorator\n",
    "\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "from langchain_core.runnables.graph import NodeColors\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from math_tools import get_math_tool\n",
    "from output_parser import LLMCompilerPlanParser, Task\n",
    "\n",
    "from concurrent.futures import wait\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'LLMCompiler'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tools\n",
    "We'll first define the tools for the agent to use in our demo. We'll give it the class search engine + calculator combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aman/anaconda3/envs/pinegap/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. You can read more about the method here: https://python.langchain.com/docs/modules/model_io/chat/structured_output/ Please follow our extraction use case documentation for more guidelines on how to do information extraction with LLMs. https://python.langchain.com/docs/use_cases/extraction/. If you notice other issues, please provide feedback here: https://github.com/langchain-ai/langchain/discussions/18154\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "calculate = get_math_tool(ChatOpenAI(model='gpt-4o'))\n",
    "search = TavilySearchResults(max_results=1, description='tavily_search_results_json(query=\"the search query\") - a search engine.')\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'89'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate.invoke({\n",
    "    'problem': 'What is the temp of sf + 57',\n",
    "    'context': ['The temperature of sf is 32 degrees']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Planner\n",
    "Largely adopted from the original source code, the planner accepts the input question and generates a task list to execute.<br>\n",
    "If it is provided with a previous plan, it is instructed to re-plan, which is useful if, upon completion of the first batch of tasks, the agent must take more actions.<br><br>\n",
    "The code below constructs the prompt template for the planner and composes it with LLM and output parser, defined in output_parser.py. The output parser processes a task list in the following form.<br>\n",
    "```plaintext\n",
    "1. tool_1(arg1='arg1', arg2=3.5, ...)\n",
    "Thought: I then want to find out Y by using tool_2\n",
    "2. tool_2(arg1='', arg2='${1}')\n",
    "3. join()<END_OF_PLAN>\n",
    "```\n",
    "\n",
    "The Thought lines are optional. The ${#} placeholders are variables. These are used to route tool (task) outputs to other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull('wfh/llm-compiler')\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate):\n",
    "    tool_descriptions = '\\n'.join(f\"{i + 1}. {tool.description}\\n\" for i, tool in enumerate(tools))\n",
    "    planner_prompt = base_prompt.partial(replan='', num_tools=len(tools) + 1, tool_descriptions=tool_descriptions)\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        '(given as Observation) of each plan and a general thought (given as Thought) about the executed results.'\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        ' - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n'\n",
    "        ' - You must continue the task index from the end of the previous one. Do not repeat task indices.',\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "    \n",
    "    def wrap_messages(state: list):\n",
    "        return {'messages': state}\n",
    "    \n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs['idx'] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f' - Begin counting at : {next_task}'\n",
    "        return {'messages': state}\n",
    "    \n",
    "    return (\n",
    "        RunnableBranch((should_replan, wrap_and_get_last_index | replanner_prompt), wrap_messages | planner_prompt)\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 {'query': 'current temperature in San Francisco'}\n",
      "----\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem\\n - `problem` can either be a simple math problem (eg \"1 + 3\") or a word problem (eg \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\"1 + 3, 2 + 4\")` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\"1 + 3\")` and then `math(\"2 + 4\")`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`.You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'pydantic.v1.main.mathSchema'> func=<function get_math_tool.<locals>.calculate_expression at 0x13bb7df30> {'problem': 'temperature in San Francisco raised to the 3rd power', 'context': ['$1']}\n",
      "----\n",
      "join ()\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "example_question = 'What is the temperature in SF raised to the 3rd power?'\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task['tool'], task['args'])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task Fetching Unit\n",
    "This component schedules the tasks. It receives a stream of tools of the following format:\n",
    "```typescript\n",
    "{\n",
    "    tool: BaseTool,\n",
    "    dependencies: number[]\n",
    "}\n",
    "```\n",
    "\n",
    "The basic idea is to begin executing tools as soon as their dependencies are met. This is done through multi-threading. We will combine the task fetching unit and executor.\n",
    "\n",
    "### LLMCompiler\n",
    "is a framework that enables an efficient and effective orchestration of parallel function calling with LLMs, including both open source and close-source models, by automatically identifying which tasks can be performed in parallel and which ones are interdependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs['idx'])] = message.content\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task['tool']\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task['args']\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {key: _resolve_arg(val, observations) for key, val in args.items()}\n",
    "        else:\n",
    "            # this will likely fail\n",
    "            resolved_args = args\n",
    "\n",
    "    except Exception as e:\n",
    "        return (f'ERROR(Failed to call {tool_to_use.name} with args {args}. Args could not be resolved. Error: {repr(e)}')\n",
    "    \n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (f'ERROR(Failed to call {tool_to_use.name} with args {args}. Args resolved to {resolved_args}. Error: {repr(e)})')\n",
    "    \n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r'\\$\\{?(\\d+)\\}?'\n",
    "\n",
    "    def replace_match(match):\n",
    "        # if the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123\n",
    "        # return the match group, in this case the index, from the string. This is the index number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "    \n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "    \n",
    "@chain_decorator\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs['task']\n",
    "    observations: Dict[int, Any] = task_inputs['observations']\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback .format_exception()\n",
    "    observations[task['idx']] = observation\n",
    "\n",
    "def schedule_pending_task(task: Task, observations: Dict[int, Any], retry_after: float=0.2):\n",
    "    while True:\n",
    "        deps = task['dependencies']\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({'task': task, 'observations': observations})\n",
    "        break\n",
    "\n",
    "@chain_decorator\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    '''Group the tasks into a DAG schedule'''\n",
    "    # for streaming we are making a few simplifying assumptions\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. The LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data-structure\n",
    "    tasks = scheduler_input['tasks']\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input['messages']\n",
    "\n",
    "    # if we are re-planning, we may have calls that depend on previous plans. Start with those\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to avoid race conditions\n",
    "    futures = []\n",
    "    retry_after = 0.25 # retry after every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task['dependencies']\n",
    "            task_names[task['idx']] = (task['tool'] if isinstance(task['tool'], str) else task['tool'].name)\n",
    "            args_for_tasks[task['idx']] = task['args']\n",
    "            if (deps and (any([dep not in observations for dep in deps]))):\n",
    "                futures.append(executor.submit(schedule_pending_task, task, observations, retry_after))\n",
    "            else:\n",
    "                # no deps, or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # all tasks have been submitted or enqueued\n",
    "        # wait for them to complete\n",
    "        wait(futures)\n",
    "\n",
    "    # convert observations to new tools messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k]) for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "\n",
    "    tool_messages = [\n",
    "        FunctionMessage(name=name, content=str(obs), additional_kwargs={'idx': k, 'args': task_args}) for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain_decorator\n",
    "def plan_and_schedule(messages: List[BaseMessage], config):\n",
    "    tasks = planner.stream(messages, config)\n",
    "    # begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # handle the case where tasks is empty\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke({'messages': messages, 'tasks': tasks}, config)\n",
    "    return scheduled_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example plan\n",
    "We still haven't introduced any cycles in our computation graph, so this is all easily expressed in LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_messages = plan_and_schedule.invoke([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content=\"[{'url': 'https://forecast.weather.gov/MapClick.php?lat=37.7749&lon=-122.4194', 'content': 'NOAA National Weather Service. Current conditions at SAN FRANCISCO DOWNTOWN (SFOC1) Lat: 37.77056°NLon: 122.42694°WElev: 150.0ft.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in San Francisco'}}, name='tavily_search_results_json'),\n",
       " FunctionMessage(content='ValueError(\\'Failed to evaluate \"${current_temperature}**3\". Raised error: SyntaxError(\\\\\\'invalid syntax\\\\\\', (\\\\\\'<expr>\\\\\\', 1, 1, \\\\\\'${current_temperature}**3\\\\\\', 1, 2)). Please try again with a valid numerical expression\\')', additional_kwargs={'idx': 2, 'args': {'problem': 'what is the temperature in San Francisco raised to the 3rd power?', 'context': ['$1']}}, name='math'),\n",
       " FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, name='join')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joiner\n",
    "So now we have the planning and initial execution done. We need a component to process these outputs and either:\n",
    "1. Respond with the correct answer\n",
    "2. Loop with a new plan\n",
    "\n",
    "<br>\n",
    "The paper refers to this as the \"joiner\". It's another LLM call. We are using function calling to improve parsing reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(description='Analysis of the previous attempts and recommendations on what needs to be fixed.')\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    '''Decide whether to replan or whether you can return the final response'''\n",
    "    thought: str = Field(description='The chain of thought reasoning for the selected action')\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "joiner_prompt = hub.pull('wfh/llm-compiler-joiner').partial(examples='') # you can optionally add examples\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "runnable = joiner_prompt | llm.with_structured_output(JoinOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f'Thought: {decision.thought}')]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return response + [SystemMessage(content=f'Context from last attempt: {decision.action.feedback}')]\n",
    "    else:\n",
    "        return response + [AIMessage(content=decision.action.response)]\n",
    "\n",
    "def select_recent_messages(messages: list) -> dict:\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {'messages': selected[::-1]}\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Thought: I attempted to find the current temperature in San Francisco and then calculate its cube, but I encountered an error. I need to successfully retrieve the current temperature first.'),\n",
       " SystemMessage(content='Context from last attempt: I need to retrieve the current temperature in San Francisco from a reliable source.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages\n",
    "joiner.invoke(input_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compose using LangGraph\n",
    "We'll define the agent as a stateful graph, with the main nodes being:\n",
    "1. Plan and execute (the DAG from the first step above)\n",
    "2. Join: determine if we should finish or replan\n",
    "3. Recontextualize: update the graph state based on the output from the joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = MessageGraph()\n",
    "\n",
    "# 1. Define vertices\n",
    "# we defined plan_and_schedule above already\n",
    "# assign each node ot a state variable to update\n",
    "graph_builder.add_node('plan_and_schedule', plan_and_schedule)\n",
    "graph_builder.add_node('join', joiner)\n",
    "\n",
    "# define edges\n",
    "graph_builder.add_edge('plan_and_schedule', 'join')\n",
    "\n",
    "# this condition determines looping logic\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if isinstance(state[-1], AIMessage):\n",
    "        return END\n",
    "    return 'plan_and_schedule'\n",
    "\n",
    "graph_builder.add_conditional_edges('join', should_continue)\n",
    "graph_builder.set_entry_point('plan_and_schedule')\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': [FunctionMessage(content='[{\\'url\\': \\'https://en.wikipedia.org/wiki/Economy_of_New_York_(state)\\', \\'content\\': \"36,000 farms occupy 7.6\\\\xa0million acres or about 25 percent of the state\\'s land area, to produce a variety of food products.[22] Here are some of the items in which New York ranks high nationally:\\\\nNew York is an agricultural leader and is one of the top five states for agricultural products, including dairy, cattle, apples, cabbages, potatoes, beets, viticulture, onions, maple syrup and many others.[23] The state is the second largest producer of cabbage in the U.S.[22] In April 2021, GlobalFoundries, a company specializing in the semiconductor industry, moved its headquarters from Silicon Valley, California to its most advanced semiconductor-chip manufacturing facility in Saratoga County near a section of the Adirondack Northway, in Malta, New York.[9]\\\\nNew York City[edit]\\\\nNew York City, characterized as the world\\'s premier financial center,[13][14][15][11] and the surrounding New York metropolitan area dominate the economy of the state. The resource capability in 2017 was 42,839 MW.[24][25] The NYISO\\'s market monitor described the average all-in wholesale electric price as a range (a single value was not provided) from $25 per MWh to $53 per MWh for 2017.[26]\\\\nSolar power[edit]\\\\nAs of mid-2023, New York has over 4,717 MW of solar power installed, generating almost 5% of the state\\'s electricity.[27]\\\\nNew York has a renewable portfolio standard of 30% from renewable sources by 2015. Manhattan contained over 500 million square feet (46.5 million m2) of office space in 2015,[16] making it the largest office market in the United States,[17] while Midtown Manhattan, with nearly 400 million square feet (37.2 million m2) in 2015,[16] is the largest central business district in the world.[18] New York is a top-tier global high technology hub.[19]\\\\nLong Island[edit]\\\\nLong Island has played a prominent role in scientific research and in engineering. The economy of the State of New York is reflected in its gross state product in 2022 of $2.053 trillion, ranking third in size behind the larger states of California and Texas.\"}]', additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, name='tavily_search_results_json', id='5b42e4c1-17bc-457f-9c8f-096fefe1f108'), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, name='join', id='6847e24d-c3d1-4b17-a6c2-3e5b990570f4')]}\n",
      "----\n",
      "{'join': [AIMessage(content=\"Thought: The GDP of the State of New York in 2022 was $2.053 trillion. This information is sufficient to answer the user's question.\", id='c533340b-8e9a-404a-af42-0526b863510b'), AIMessage(content='The GDP of the State of New York in 2022 was $2.053 trillion.', id='34aa7342-29bc-4f4a-83ef-32560fc65fb8')]}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream([HumanMessage(content=\"What's the GDP of New York?\")]):\n",
    "    print(step)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The GDP of the State of New York in 2022 was $2.053 trillion.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step['join'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-hop question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': [FunctionMessage(content='[{\\'url\\': \\'https://www.guinnessworldrecords.com/world-records/442525-oldest-parrot-ever\\', \\'content\\': \"Oldest parrot ever. The oldest parrot ever is Cookie, a Major Mitchell\\'s cockatoo (Cacatua leadbeateri) who was at least 82 years and 88 days old when he passed away on 27 August 2016. Cookie\\'s exact age was unknown when he arrived at Brookfield Zoo in May 1934. His arrival was documented in a ledger dated May 1934, when he was estimated to ...\"}]', additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive'}}, name='tavily_search_results_json', id='22f91ac3-29c5-461d-af47-c69dcf8df7b4'), FunctionMessage(content='[{\\'url\\': \\'https://www.thesprucepets.com/how-long-do-parrots-and-other-pet-birds-live-1238433\\', \\'content\\': \"It\\'s possible that a pet bird can outlive its owners\\\\nThe Spruce / Adrienne Legault\\\\nParrots and other birds can live up to 10 to 50 years or more depending on the type and the conditions they live in. They vary in size from small birds that can fit in the palm of your hand to large birds the size of a cat and their lifespans are just as variable.\\\\n Also, for birds who live longer some owners have to make a plan of where the bird is going in the circumstance the bird outlives the owner.\\\\n In reality, there is a wide range in the age that pet birds might reach and certainly, some will live longer (or shorter amounts of time) than the ages listed.\\\\n Potential owners need to be aware of the longevity of their bird so they can be prepared to provide proper care for them for as long as they live.\\\\n\"}]', additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of a parrot'}}, name='tavily_search_results_json', id='8fff9906-9fce-45ca-8c8b-f2367cb02bce'), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, name='join', id='8fe168d1-9c40-4227-b143-6d101913f22e')]}\n",
      "----\n",
      "{'join': [AIMessage(content='Thought: The oldest known parrot was Cookie, who lived to be at least 82 years and 88 days old. Parrots generally live between 10 to 50 years, depending on the species and conditions. Therefore, Cookie lived significantly longer than the average parrot.', id='66ea44c9-b49a-416b-9f77-307143b84d77'), AIMessage(content=\"The oldest known parrot was Cookie, a Major Mitchell's cockatoo, who lived to be at least 82 years and 88 days old. The average lifespan of parrots ranges from 10 to 50 years, so Cookie lived significantly longer than the average parrot.\", id='f808f370-3d01-4c80-9903-9a979d9a6854')]}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "steps = chain.stream([HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")], {'recursion_limit': 100})\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The oldest known parrot was Cookie, a Major Mitchell's cockatoo, who lived to be at least 82 years and 88 days old. The average lifespan of parrots ranges from 10 to 50 years, so Cookie lived significantly longer than the average parrot.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step['join'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-step math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': [FunctionMessage(content='None', additional_kwargs={'idx': 1, 'args': {'problem': '3*(4+5)/0.5+3245+8'}}, name='math', id='59533f8e-111e-4b2f-b907-f19f41baa394'), FunctionMessage(content='None', additional_kwargs={'idx': 2, 'args': {'problem': '32/4.23'}}, name='math', id='9d123b3f-f9f3-426f-8509-d84d16d91783'), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, name='join', id='df950090-a788-45b5-8790-153222e57c35')]}\n",
      "{'join': [AIMessage(content='Thought: I need to perform the calculations to find the values and their sum as requested by the user.', id='aff1ffe9-7e39-4e67-ac5a-ebe27b89d4cc'), SystemMessage(content='Context from last attempt: I need to perform the calculations to find the values and their sum as requested by the user.', id='29309ac3-bcfe-40f5-b9d0-b11b438b359d')]}\n",
      "{'plan_and_schedule': [FunctionMessage(content='None', additional_kwargs={'idx': 4, 'args': {'problem': '3*(4+5)/0.5 + 3245 + 8'}}, name='math', id='793bcc12-e2d6-4774-9e4a-763b54aa455b'), FunctionMessage(content='None', additional_kwargs={'idx': 5, 'args': {'problem': '32/4.23'}}, name='math', id='51bb24da-b706-4677-abf8-7e24c1bf8a5f'), FunctionMessage(content='ValueError(\\'Failed to evaluate \"None\". Raised error: TypeError(\"unsupported expression type: <class \\\\\\'NoneType\\\\\\'>\"). Please try again with a valid numerical expression\\')', additional_kwargs={'idx': 6, 'args': {'problem': 'sum of $4 and $5', 'context': ['$4', '$5']}}, name='math', id='20882734-84aa-44c0-9608-779b5b2799b8'), FunctionMessage(content='join', additional_kwargs={'idx': 7, 'args': ()}, name='join', id='b946dc4f-c83a-4aa6-ac57-ac337b73a144')]}\n",
      "{'join': [AIMessage(content='Thought: I need to perform the calculations for the given expressions and their sum, as the previous attempts failed.', id='fe07e4c6-52cd-4cb7-a0f2-24aa94712bca'), SystemMessage(content='Context from last attempt: Previous attempts to evaluate the expressions failed. I need to replan and correctly calculate the expressions.', id='9228d093-4d18-4bad-8ae1-65f6c9c5be4d')]}\n",
      "{'plan_and_schedule': []}\n",
      "{'join': [AIMessage(content='Thought: I need to correctly calculate the expressions: ((3*(4+5)/0.5)+3245) + 8 and 32/4.23, and then find their sum.', id='1bb96dc8-b897-41ad-8640-82851765548a'), SystemMessage(content='Context from last attempt: Previous attempts to evaluate the expressions failed. I need to replan and correctly calculate the expressions.', id='9de2a809-56bc-45f5-ac5b-8a9c14b263d7')]}\n",
      "{'plan_and_schedule': [FunctionMessage(content='None', additional_kwargs={'idx': 8, 'args': {'problem': '((3*(4+5)/0.5)+3245) + 8'}}, name='math', id='a112344d-ab76-4143-8347-2263b9faaf34'), FunctionMessage(content='None', additional_kwargs={'idx': 9, 'args': {'problem': '32/4.23'}}, name='math', id='397e9217-e35a-47e6-a789-1dad114ad137'), FunctionMessage(content='join', additional_kwargs={'idx': 10, 'args': ()}, name='join', id='079a372b-2af8-4896-8c13-ae68515d1313')]}\n",
      "{'join': [AIMessage(content='Thought: The previous attempts failed due to issues in evaluating the expressions. I will attempt to replan and accurately calculate the expressions.', id='b5828d39-019f-4f02-aaac-e2cf8ffc8ab3'), SystemMessage(content='Context from last attempt: The previous attempts to evaluate the expressions were unsuccessful. I will now replan and accurately calculate the expressions: ((3*(4+5)/0.5)+3245) + 8 and 32/4.23, and then find their sum.', id='f98be615-e8d6-40a4-ae01-b1602398761d')]}\n",
      "{'plan_and_schedule': []}\n",
      "{'join': [AIMessage(content='Thought: The previous attempts to evaluate the expressions were unsuccessful. I will now replan and accurately calculate the expressions: ((3*(4+5)/0.5)+3245) + 8 and 32/4.23, and then find their sum.', id='418cbd30-3856-4057-864f-2a88c725114f'), SystemMessage(content='Context from last attempt: The previous attempts to evaluate the expressions were unsuccessful. I will now replan and accurately calculate the expressions: ((3*(4+5)/0.5)+3245) + 8 and 32/4.23, and then find their sum.', id='b9f21447-d65f-457d-8c15-c5cb863c5aa8')]}\n",
      "{'plan_and_schedule': [FunctionMessage(content='None', additional_kwargs={'idx': 11, 'args': {'problem': '(3*(4+5)/0.5)+3245 + 8'}}, name='math', id='cd73f590-4512-45ac-979d-bde1a5363c03'), FunctionMessage(content='None', additional_kwargs={'idx': 12, 'args': {'problem': '32/4.23'}}, name='math', id='a9e93330-36b1-4a8c-b2ce-62d6e92bec83'), FunctionMessage(content='ValueError(\\'Failed to evaluate \"None + None\". Raised error: TypeError(\"unsupported operand type(s) for +: \\\\\\'NoneType\\\\\\' and \\\\\\'NoneType\\\\\\'\"). Please try again with a valid numerical expression\\')', additional_kwargs={'idx': 13, 'args': {'problem': '($11 + $12)', 'context': ['$11', '$12']}}, name='math', id='a30ad2fb-1c8f-4891-8b8e-7789f1141bcc'), FunctionMessage(content='join', additional_kwargs={'idx': 14, 'args': ()}, name='join', id='b7ac48e6-84bd-479c-a076-4c8392a809ba')]}\n",
      "{'join': [AIMessage(content='Thought: Repeated attempts to directly evaluate the expressions have failed. I should acknowledge the difficulty and explain the steps the user can take to solve these calculations manually.', id='b964e4bd-bafe-4df6-a7c5-6303a836d979'), AIMessage(content='I encountered difficulties in directly evaluating the expressions. Here are the steps you can follow to solve them manually:\\n\\n1. Calculate the first expression: ((3*(4+5)/0.5)+3245) + 8\\n   - First, solve inside the parentheses: (4+5) = 9\\n   - Then multiply by 3: 3 * 9 = 27\\n   - Divide by 0.5: 27 / 0.5 = 54\\n   - Add 3245: 54 + 3245 = 3299\\n   - Finally, add 8: 3299 + 8 = 3307\\n\\n2. Calculate the second expression: 32 / 4.23\\n   - Perform the division: 32 / 4.23 ≈ 7.57\\n\\n3. Sum the two results: 3307 + 7.57 = 3314.57\\n\\nSo, the final sum is approximately 3314.57.', id='2786e2a2-d10f-469d-b7c0-56b822b66369')]}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream([HumanMessage(content=\"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\")]):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I encountered difficulties in directly evaluating the expressions. Here are the steps you can follow to solve them manually:\n",
      "\n",
      "1. Calculate the first expression: ((3*(4+5)/0.5)+3245) + 8\n",
      "   - First, solve inside the parentheses: (4+5) = 9\n",
      "   - Then multiply by 3: 3 * 9 = 27\n",
      "   - Divide by 0.5: 27 / 0.5 = 54\n",
      "   - Add 3245: 54 + 3245 = 3299\n",
      "   - Finally, add 8: 3299 + 8 = 3307\n",
      "\n",
      "2. Calculate the second expression: 32 / 4.23\n",
      "   - Perform the division: 32 / 4.23 ≈ 7.57\n",
      "\n",
      "3. Sum the two results: 3307 + 7.57 = 3314.57\n",
      "\n",
      "So, the final sum is approximately 3314.57.\n"
     ]
    }
   ],
   "source": [
    "print(step['join'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Known limitations to the implementation above:\n",
    "1. The planner output parsing format is fragile if your function requires more than 1 or 2 arguments. We could make it more robust by using streaming tool calling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinegap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
