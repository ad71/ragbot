{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORM\n",
    "### Synthesis of Topic Outlines through Retrieval and Multi-perspective question asking.\n",
    "\n",
    "Research assistant that extends the idea of 'outline-driven RAG' for richer article generation.\n",
    "\n",
    "It applies two main insights to produce more organized and comprehensive articles:\n",
    "1. Creating an outline (planning) by querying similar topics helps improve coverage.\n",
    "2. Multi-perspective, grounded (in search) conversation simulation helps increase the reference count and information density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "1. Generate initial outline + survey related subjects\n",
    "2. Identify distiinct perspectives\n",
    "3. \"Interview subject matter experts\" (role-playing LLMs)\n",
    "4. Refine outline (using references)\n",
    "5. Write sections, then write article\n",
    "\n",
    "The expert interviews stage occurs between the role-playing article writer and a research expert. The \"expert\" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article.\n",
    "\n",
    "Hyperparameters to restrict the potentially infinite research breadth:\n",
    "- N: number of perspectives to survey\n",
    "- M: Max number of conversation turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select LLMs\n",
    "We will have a faster LLM to do most of the work, but a slower, long-context model to distill the conversations and write the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "FAST_LLM = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "GOOD_LLM = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Initial Outline\n",
    "For many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial outline to be referred after our research. Below, we will use our 'fast' llm to generate the outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AnyMessage, AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig, chain as as_runnable\n",
    "\n",
    "from typing import List, Optional, Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'You are a an experienced equity research analyst. Write an outline for an analysis report about a user-provided topic. Be comprehensive and specific.'\n",
    "        ),\n",
    "        ('user', '{topic}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title='Title of the subsection')\n",
    "    description: str = Field(..., title='Content of the subsection')\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f'### {self.subsection_title}\\n\\n{self.description}'.strip()\n",
    "    \n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title='Title of the section')\n",
    "    description: str = Field(..., title='Content of the section')\n",
    "    subsections: Optional[List[Subsection]] = Field(default=None, title='Titles and descriptions for each subsection of the analysis report.')\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = '\\n\\n'.join(f'### {subsection.subsection_title}\\n\\n{subsection.description}' for subsection in self.subsections or [])\n",
    "        return f'## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}'.strip()\n",
    "    \n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title='Title of the Report')\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title='Titles and descriptions for each section of the Report page.'\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = '\\n\\n'.join(section.as_str for section in self.sections)\n",
    "        return f'# {self.page_title}\\n\\n{sections}'.strip()\n",
    "    \n",
    "generate_outline_direct = direct_gen_outline_prompt | FAST_LLM.with_structured_output(Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_topic = 'Impact on silicon industry due to the China Taiwan war'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_outline = generate_outline_direct.invoke({'topic': example_topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analysis Report: Impact on Silicon Industry due to China-Taiwan Conflict\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Brief overview of the China-Taiwan conflict and its potential implications on the silicon industry.\n",
      "\n",
      "## Background of the Silicon Industry\n",
      "\n",
      "Overview of the global silicon industry, key players, market size, and growth trends.\n",
      "\n",
      "## Current Market Dynamics\n",
      "\n",
      "Analysis of the current market conditions in the silicon industry pre-conflict.\n",
      "\n",
      "## Potential Supply Chain Disruptions\n",
      "\n",
      "Assessment of potential supply chain disruptions in the silicon industry due to the China-Taiwan conflict.\n",
      "\n",
      "## Impact on Key Players\n",
      "\n",
      "Evaluation of how major players in the silicon industry may be affected by the conflict.\n",
      "\n",
      "## Market Volatility and Investment Opportunities\n",
      "\n",
      "Discussion on the potential market volatility and investment opportunities in the silicon industry.\n",
      "\n",
      "## Government Policies and Regulatory Changes\n",
      "\n",
      "Analysis of how government policies and regulatory changes may impact the silicon industry in response to the conflict.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Summary of key findings and recommendations for stakeholders in the silicon industry.\n"
     ]
    }
   ],
   "source": [
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand topics\n",
    "While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine.\n",
    "\n",
    "We will start our search by generating a list of related topics, sourced from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    '''I'm writing a financial report for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects.\n",
    "    I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
    "    Please list as many subjects and urls as you can.\n",
    "\n",
    "    Topic of interest: {topic}''')\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(description='Comprehensive list of related subjects as background research')\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | FAST_LLM.with_structured_output(RelatedSubjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Silicon industry', 'China-Taiwan conflict', 'Technology impact', 'Economic impact'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({'topic': example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wikipedia editors as 'subject matter experts' with distinct backgrounds and affiliations. These will help distribute the research process to encourage a more well-rounded final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(description='Primary affiliation of the editor.')\n",
    "    name: str = Field(description='Name of the editor', pattern=r'^[a-zA-Z0-9_-]{1,64}$')\n",
    "    role: str = Field(description='Role of the editor in the context of the topic.')\n",
    "    description: str = Field(description='Description of the editor\\'s focus, concerns and motives')\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f'Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n'\n",
    "    \n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(description='Comprehensive list of editors with their roles and affiliations.')\n",
    "\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            '''You need to select a diverse (and distinct) group of equity research analysts, equity researchers and editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic. \n",
    "            You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on. \n",
    "            \n",
    "            Wiki pages outlines of related topics for inspiration: \n",
    "            {examples}''',\n",
    "        ),\n",
    "        (\n",
    "            'user'\n",
    "            'Topic of interest: {topic}'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | FAST_LLM.with_structured_output(Perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = '- '.join(doc.metadata['categories'])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[:max_length]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(format_doc(doc) for doc in docs)\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({'topic': topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(related_subjects.topics, return_exceptions=True)\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({'examples': formatted, 'topic': topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editors=[Editor(affiliation='Equity Research Firm', name='AliceSmith', role='Equity Research Analyst', description='Alice will focus on analyzing the potential impact of the China-Taiwan conflict on the silicon industry, including supply chain disruptions, market sentiment, and stock performance of key companies in the sector.'), Editor(affiliation='Tech News Outlet', name='JohnDoe', role='Tech Editor', description='John will provide insights on the technological advancements in the silicon industry and how the conflict may influence innovation, research, and development in the sector.'), Editor(affiliation='Global Economic Magazine', name='EmmaBrown', role='Economic Analyst', description='Emma will examine the macroeconomic implications of the China-Taiwan conflict on the global silicon industry, including trade dynamics, investment trends, and economic forecasts.')]\n"
     ]
    }
   ],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Equity Research Firm',\n",
       "   'name': 'AliceSmith',\n",
       "   'role': 'Equity Research Analyst',\n",
       "   'description': 'Alice will focus on analyzing the potential impact of the China-Taiwan conflict on the silicon industry, including supply chain disruptions, market sentiment, and stock performance of key companies in the sector.'},\n",
       "  {'affiliation': 'Tech News Outlet',\n",
       "   'name': 'JohnDoe',\n",
       "   'role': 'Tech Editor',\n",
       "   'description': 'John will provide insights on the technological advancements in the silicon industry and how the conflict may influence innovation, research, and development in the sector.'},\n",
       "  {'affiliation': 'Global Economic Magazine',\n",
       "   'name': 'EmmaBrown',\n",
       "   'role': 'Economic Analyst',\n",
       "   'description': 'Emma will examine the macroeconomic implications of the China-Taiwan conflict on the global silicon industry, including trade dynamics, investment trends, and economic forecasts.'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert Dialog\n",
    "Each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generates content to generate a refined outline as well as an updated index of reference documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview State\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialog roles\n",
    "The graph will have two participants: the wikipedia editor (`generate_question`), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain`), who uses a search engine to answer the questions as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            '''You are an experienced equity research analyst with great critical thinking skills. \n",
    "            You want to edit a specific report. \n",
    "            Besides your exceptional writing skills, you have a specific focus when researching the topic. \n",
    "            Now you are chatting with an expert to get information. Ask good questions to get more useful information. \n",
    "            Make sure to get answers for specific industries, specific companies or ticker symbols and verticals within companies, as and when relevant. \n",
    "            \n",
    "            When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation. \n",
    "            Please only ask one question at a time and don't ask what you have asked before. \n",
    "            Be comprehensive and curious, gaining as much unique insight from the expert as possible.\n",
    "            \n",
    "            Stay true to your specific perspective: \n",
    "            \n",
    "            {persona}''',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages', optional=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={'type'}))\n",
    "        converted.append(message)\n",
    "    return {'messages': converted}\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state['editor']\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | FAST_LLM\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {'messages': [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, that's correct. I am analyzing the potential impact of the China-Taiwan conflict on the silicon industry, including supply chain disruptions, market sentiment, and stock performance of key companies in the sector. Do you have any insights on how this conflict might affect specific companies within the silicon industry?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f'So you said you were writing an article on {example_topic}?')\n",
    "]\n",
    "\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        'editor': perspectives.editors[0],\n",
    "        'messages': messages\n",
    "    }\n",
    ")\n",
    "\n",
    "question['messages'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Questions\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(description='Comprehensive list of search engine queries to answer the user\\'s question')\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'You are a helpful equity research assistant. Query the search engine to answer the user\\'s questions.'\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages', optional=True)\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | GOOD_LLM.with_structured_output(Queries, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['impact of China-Taiwan conflict on silicon industry',\n",
       " 'China-Taiwan conflict effect on silicon supply chain',\n",
       " 'market sentiment on silicon stocks due to China-Taiwan tensions',\n",
       " 'China-Taiwan conflict and stock performance of silicon companies',\n",
       " 'key silicon companies affected by China-Taiwan conflict']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke({'messages': [HumanMessage(content=question['messages'][0].content)]})\n",
    "queries['parsed'].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswersWithCitations(BaseModel):\n",
    "    answer: str = Field(description='Comprehensive answer to the user\\'s question with citations.')\n",
    "    cited_urls: List[str] = Field(\n",
    "        description='List of urls cited in the answer.',\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f'{self.answer}\\n\\nCitations:\\n\\n' + '\\n'.join(\n",
    "            f'[{i+1}: {url}]' for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "    \n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            '''You are an expert equity research analyst who can use information effectively. \n",
    "            You are chatting with an equity research report writer who wants to write an analysis report on the topic you know. \n",
    "            You have gathered the related information and will now use the information to form a response. \n",
    "            \n",
    "            Make your response as informative as possible and make sure every sentence is supported by the gathered information. \n",
    "            Be sure to mention specific industries, companies and verticals within companies as required. \n",
    "            Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLs after your response.'''\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages', optional=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | GOOD_LLM.with_structured_output(AnswersWithCitations, include_raw=True).with_config(run_name='GenerateAnswer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily is typically a better search engine but free queries are limited.\n",
    "# search_engine = TavilySearchResults(max_results=4)\n",
    "\n",
    "# @tool\n",
    "# async def search_engine(query: str):\n",
    "#     '''Search engine to the internet'''\n",
    "#     results = tavily_search.invoke(query)\n",
    "#     return [{'content': r['content'], 'url': r['url']} for r in results]\n",
    "\n",
    "# DDG\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    '''Search engine to the internet'''\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{'content': r['body'], 'url': r['href']} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gen_answer(state: InterviewState, config: Optional[RunnableConfig]=None, name: str='subject_matter_expert', max_str_len: int=15000):\n",
    "    swapped_state = swap_roles(state, name) # convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(queries['parsed'].queries, config, return_exceptions=True)\n",
    "    successful_results = [res for res in query_results if not isinstance(res, Exception)]\n",
    "    all_query_results = {res['url']: res['content'] for results in successful_results for res in results}\n",
    "\n",
    "    # we could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries['raw']\n",
    "    tool_call = queries['raw'].additional_kwargs['tool_calls'][0]\n",
    "    tool_id = tool_call['id']\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state['messages'].extend([ai_message, tool_message])\n",
    "\n",
    "    # only update the shared state with the final answer to avoid polluting the dialog history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated['parsed'].cited_urls)\n",
    "\n",
    "    # save the retrieved information to the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated['parsed'].as_str)\n",
    "    return {'messages': [formatted_message], 'references': cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_answer = await gen_answer(\n",
    "    {'messages': [HumanMessage(content=question['messages'][0].content)]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The China-Taiwan conflict poses significant risks to the silicon industry, impacting supply chains, market sentiment, and the stock performance of key companies. Taiwan is a critical hub for semiconductor manufacturing, with companies like Taiwan Semiconductor Manufacturing Company (TSMC) being central to the global supply of advanced chips. TSMC alone produces about 90% of the world's most advanced semiconductors[^1^]. Any disruption in Taiwan due to geopolitical tensions could severely impact global tech industries reliant on these chips, including sectors like consumer electronics, automotive, and telecommunications[^2^]. This dependency creates vulnerabilities, as highlighted by the global disruptions during the COVID-19 pandemic[^3^]. For instance, an escalation in the China-Taiwan conflict could force companies to activate contingency plans, significantly disrupting production and supply chains[^4^]. Additionally, US-China tensions have already led to export controls on advanced semiconductor technologies to China, further complicating the market dynamics[^5^]. Companies like Apple and Nvidia, which rely heavily on TSMC for their high-performance chips, could face stock market volatility and potential supply shortages[^6^]. Market sentiment could also be negatively affected, as investors might anticipate disruptions and react to increased geopolitical risks[^7^]. In summary, the China-Taiwan conflict has the potential to cause widespread disruptions in the silicon industry, affecting supply chains, market sentiment, and stock performance of key players in the sector.\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1: https://foreignpolicy.com/2024/04/11/semiconductor-chips-taiwan-earthquake-tsmc-choke-point/]\n",
      "[2: https://www.vox.com/technology/2024/4/4/24120498/taiwan-hualien-earthquake-magnitude-tsmc-semiconductor-china-natural-disaster-global-economy]\n",
      "[3: https://globaltaiwan.org/2023/12/despite-chinese-market-controls-taiwans-semiconductor-supply-chain-remains-secure/]\n",
      "[4: https://www.edwardconard.com/macro-roundup/both-asml-and-tsmc-have-contingency-plans-to-disable-taiwanese-chip-manufacturing-in-the-event-of-a-chinese-invasion-of-taiwan-about-90-of-the-worlds-most-advanced-chips-are-made-in-taiwan-market/?view=detail]\n",
      "[5: https://www.bakerinstitute.org/research/silicon-hegemon-could-china-take-over-taiwans-semiconductor-industry-without-invading]\n",
      "[6: https://www.investors.com/news/taiwan-presidential-race-set-us-china-chip-war-nears-moment-of-truth-for-nvidia-apple-and-the-world/]\n",
      "[7: https://seekingalpha.com/article/4667107-investors-should-brace-for-impact-if-rising-china-taiwan-tensions-escalate]\n"
     ]
    }
   ],
   "source": [
    "print(example_answer['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Interview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "def route_messages(state: InterviewState, name: str='subject_matter_expert'):\n",
    "    messages = state['messages']\n",
    "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and m.name == name])\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if last_question.content.endswith('Thank you so much for your help!'):\n",
    "        return END\n",
    "    \n",
    "    return 'ask_question'\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node('ask_question', generate_question)\n",
    "builder.add_node('answer_question', gen_answer)\n",
    "builder.add_conditional_edges('answer_question', route_messages)\n",
    "builder.add_edge('ask_question', 'answer_question')\n",
    "builder.add_edge(START, 'ask_question')\n",
    "interview_graph = builder.compile().with_config(run_name='Conduct Interviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinegap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
