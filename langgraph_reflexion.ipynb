{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflexion\n",
    "Is an architecture designed to learn through verbal feedback and self-reflection. The agent explicitly critiques its reponses for tasks to generate a higher quality final response, at the expense of longer execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import asyncio\n",
    "import datetime\n",
    "import platform\n",
    "import requests\n",
    "import operator\n",
    "import playwright\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import MessageGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_executor import ToolInvocation\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.function import FunctionMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.pydantic_v1 import ValidationError\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "from langchain_core.runnables.graph import NodeColors\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "from langchain_fireworks.chat_models import ChatFireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Reflexion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools are invoked _in context_. Create a function that invokes all the requested tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper class we have that is useful for running tools\n",
    "# it takes in an agent action and calls that tool and returns the result\n",
    "tool_executor = ToolExecutor([tavily_tool])\n",
    "# parse the tool messages for the execution / invocation\n",
    "parser = JsonOutputToolsParser(return_id=True)\n",
    "\n",
    "def execute_tools(state: List[BaseMessage]) -> List[BaseMessage]:\n",
    "    tool_invocation: AIMessage = state[-1]\n",
    "    parsed_tool_calls = parser.invoke(tool_invocation)\n",
    "    ids = []\n",
    "    tool_invocations = []\n",
    "    for parsed_call in parsed_tool_calls:\n",
    "        for query in parsed_call['args']['search_queries']:\n",
    "            tool_invocations.append(\n",
    "                ToolInvocation(\n",
    "                    # we only have this one for now. Would want to map it if we change\n",
    "                    tool = 'tavily_search_results_json',\n",
    "                    tool_input='query'\n",
    "                )\n",
    "            )\n",
    "            ids.append(parsed_call['id'])\n",
    "\n",
    "    outputs = tool_executor.batch(tool_invocations)\n",
    "    outputs_map = defaultdict(dict)\n",
    "    for id_, output, invocation in zip(ids, outputs, tool_invocations):\n",
    "        outputs_map[id_][invocation.tool_input] = output\n",
    "    \n",
    "    return [ToolMessage(content=json.dumps(query_outputs), tool_call_id=id_) for id_, query_outputs in outputs_map.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_prompt = '''You are an expert researcher. \n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. Recommend search queries to research information and improve your answer.'''\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            actor_prompt\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "        ('system', 'Answer the user\\'s question above using the required format.')\n",
    "    ]\n",
    ").partial(time=lambda: datetime.datetime.now().isoformat())\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description='Critique of what is missing')\n",
    "    superfluous: str = Field(description='Critique of what is superfluous')\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    '''Answer the question'''\n",
    "    answer: str = Field(description='~250 word detailed answer to the question.')\n",
    "    reflection: Reflection = Field(description='Your reflection on the initial answer.')\n",
    "    search_queries: List[str] = Field(description='1 - 3 search queries for researching improvements to address the critique of your current answer.')\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo')\n",
    "initial_answer_chain = actor_prompt_template.partial(first_instruction='Provide a detailed ~250 word answer.') | llm.bind_tools(tools=[AnswerQuestion], tool_choice='AnswerQuestion')\n",
    "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "\n",
    "class ResponderWithRetries:\n",
    "    def __init__(self, runnable, validator):\n",
    "        self.runnable = runnable\n",
    "        self.validator = validator\n",
    "\n",
    "    @traceable\n",
    "    def respond(self, state: List[BaseMessage]):\n",
    "        response = []\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = self.runnable.invoke({'messages': state})\n",
    "                self.validator.invoke(response)\n",
    "                return response\n",
    "            except ValidationError as e:\n",
    "                print('RETRYING', attempt)\n",
    "                state = state + [HumanMessage(content=repr(e))]\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_responder = ResponderWithRetries(runnable=initial_answer_chain, validator=validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = 'Why is reflection useful in AI?'\n",
    "initial = first_responder.respond([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parser.invoke(initial)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision\n",
    "The second part of the actor is a revision step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_instructions = '''Revise your previous answer using the new information. \n",
    "    - You should use the previous critique to add important information to your answer. \n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified. \n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of: \n",
    "            - [1] https://example.com\n",
    "            - [2] https://example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.'''\n",
    "\n",
    "# extend the initial answer schema to include references\n",
    "# forcing citation in the model encourages grounded responses\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    '''Revise your original answer to the question'''\n",
    "    references: List[str] = Field(description='Citations motivating your updated answer')\n",
    "\n",
    "revision_chain = actor_prompt_template.partial(first_instruction=revise_instructions) | llm.bind_tools(tools=[ReviseAnswer], tool_choice='ReviseAnswer')\n",
    "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
    "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised = revisor.respond(\n",
    "    [\n",
    "        HumanMessage(content=''),\n",
    "        initial,\n",
    "        ToolMessage(\n",
    "            tool_call_id=initial.additional_kwargs['tool_calls'][0]['id'],\n",
    "            content=json.dumps(\n",
    "                tavily_tool.invoke(str(parsed[0]['args']['search_queries']))\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parser.invoke(revised)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 5\n",
    "builder = MessageGraph()\n",
    "builder.add_node('draft', first_responder.respond)\n",
    "builder.add_node('execute_tools', execute_tools)\n",
    "builder.add_node('revise', revisor.respond)\n",
    "\n",
    "builder.add_edge('draft', 'execute_tools') # draft -> execute_tools\n",
    "builder.add_edge('execute_tools', 'revise') # execute_tools -> revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit loop\n",
    "def _get_num_iterations(state: List[BaseMessage]):\n",
    "    i = 0\n",
    "    for m in state[::-1]:\n",
    "        if not isinstance(m, (ToolMessage, AIMessage)):\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def event_loop(state: List[BaseMessage]) -> str:\n",
    "    # in our case we'll just stop after N plans\n",
    "    num_iterations = _get_num_iterations(state)\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return 'execute_tools'\n",
    "\n",
    "# revise -> execute_tools OR end\n",
    "builder.add_conditional_edges('revise', event_loop)\n",
    "builder.set_entry_point('draft')\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRYING 0\n",
      "## 1. draft\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_5sIG2uSL6qwsAHqeplrr24uW', 'function': {'a ...\n",
      "---\n",
      "## 2. execute_tools\n",
      "[ToolMessage(content='{\"query\": [{\"url\": \"https://www.dictionary.com/browse/query\", \"content\": \"to m ...\n",
      "---\n",
      "## 3. revise\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_dmJ8BKh8JvMz4cjvaCJPI2zL', 'function': {'a ...\n",
      "---\n",
      "## 4. execute_tools\n",
      "[ToolMessage(content='{\"query\": [{\"url\": \"https://www.dictionary.com/browse/query\", \"content\": \"to m ...\n",
      "---\n",
      "## 5. revise\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_QsRKwD36t9KrLH9TzVOmW8H9', 'function': {'a ...\n",
      "---\n",
      "## 6. execute_tools\n",
      "[ToolMessage(content='{\"query\": [{\"url\": \"https://www.vocabulary.com/dictionary/query\", \"content\": \" ...\n",
      "---\n",
      "## 7. revise\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_q5x9PYUdhr3VBttyMzh9Ylms', 'function': {'a ...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(\n",
    "    [HumanMessage(content='How should we handle the climate crisis?')]\n",
    ")\n",
    "\n",
    "for i, step in enumerate(events):\n",
    "    node, output = next(iter(step.items()))\n",
    "    print(f'## {i + 1}. {node}')\n",
    "    print(str(output)[:100] + ' ...')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinegap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
