{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfUXquF2CgJuTogfvnOT1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad71/ragbot/blob/master/multi_agent_supervisor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i65a-5EZLdbq",
        "outputId": "f976e318-d1e8-478a-fe43-3a9573f9290f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m844.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_openai langchain_community langchain_experimental langsmith pandas matplotlib langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# optional, add tracing in LangSmith\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_PROJECT'] = 'Multi-Agent Collaboration'"
      ],
      "metadata": {
        "id": "5NKnLHD2Lii9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import operator\n",
        "import functools\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    ChatMessage,\n",
        "    HumanMessage,\n",
        "    FunctionMessage\n",
        ")\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "id": "KmnldkTCLk3N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "python_tool = PythonREPLTool()"
      ],
      "metadata": {
        "id": "gr9IfLpzLoR9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            ('system', system_prompt),\n",
        "            MessagesPlaceholder(variable_name='messages'),\n",
        "            MessagesPlaceholder(variable_name='agent_scratchpad')\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor"
      ],
      "metadata": {
        "id": "GWLPy9uSOzFo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_node(state, agent, name):\n",
        "    # convert agent result which is an AI message into a Human message with a name\n",
        "    result = agent.invoke(state)\n",
        "    return {'messages': [HumanMessage(content=result['output'], name=name)]}"
      ],
      "metadata": {
        "id": "ELHFLRyXPgRq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Agent Supervisor\n",
        "Use function calling to choose the next worker node or finish processing"
      ],
      "metadata": {
        "id": "W__Vr0czQLSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "members = ['Researcher', 'Coder']\n",
        "system_prompt = (\n",
        "    'You are a supervisor tasked with managing a conversation between the '\n",
        "    'following workers: {members}. Given the following user request, '\n",
        "    'respond with the worker to act next. Each worker will perform a '\n",
        "    'task and respond with their results and status. When finished, '\n",
        "    'respond with FINISH.'\n",
        ")\n",
        "\n",
        "# our team supervisor is an LLM node. It just picks the next agent to process and decides when the work in completed\n",
        "options = ['FINISH'] + members\n",
        "\n",
        "# using openai function calling can make output parsing easier for us\n",
        "function_def = {\n",
        "    'name': 'route',\n",
        "    'description': 'Select the next role.',\n",
        "    'parameters': {\n",
        "        'title': 'routeSchema',\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "            'next': {\n",
        "                'title': 'Next',\n",
        "                'anyOf': [\n",
        "                    {'enum': options},\n",
        "                ]\n",
        "            }\n",
        "        },\n",
        "        'required': ['next']\n",
        "    }\n",
        "}\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', system_prompt),\n",
        "        MessagesPlaceholder(variable_name='messages'),\n",
        "        (\n",
        "            'system',\n",
        "            'Given the conversation above, who should act next? '\n",
        "            'Or should we FINISH? Select one of: {options}'\n",
        "        )\n",
        "    ]\n",
        ").partial(options=str(options), members=', '.join(members))\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4-1106-preview')\n",
        "supervisor_chain = prompt | llm.bind_functions(functions=[function_def], function_call='route') | JsonOutputFunctionsParser()"
      ],
      "metadata": {
        "id": "aFgBRaHfP-uV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create graph\n",
        "class AgentState(TypedDict):\n",
        "    # the annotation tells the graph that new messages will always be added to the current states\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    # the next field indicates where to route to next\n",
        "    next: str\n",
        "\n",
        "research_agent = create_agent(llm, [tavily_tool], 'You are a web researcher.')\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name='Researcher')\n",
        "\n",
        "# WARNING: This performs arbitrary code execution. Proceed with CAUTION\n",
        "code_agent = create_agent(llm, [python_tool], 'You may generate safe python code to analyze data and generate charts using matplotlib')\n",
        "code_node = functools.partial(agent_node, agent=code_agent, name='Coder')\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node('Researcher', research_node)\n",
        "workflow.add_node('Coder', code_node)\n",
        "workflow.add_node('supervisor', supervisor_chain)"
      ],
      "metadata": {
        "id": "v_GOf1p2Wogn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want our workers to always report back to the supervisor when done\n",
        "for member in members:\n",
        "    workflow.add_edge(member, 'supervisor')\n",
        "\n",
        "# the supervisor populates the 'next' field in the graph state which routes to a node or finishes\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map['FINISH'] = END\n",
        "\n",
        "workflow.add_conditional_edges('supervisor', lambda x: x['next'], conditional_map)\n",
        "workflow.set_entry_point('supervisor')\n",
        "\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "mFhBVSAHXg22"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke the team\n",
        "\n",
        "for s in graph.stream({'messages': [HumanMessage(content='Code hello world and print it to the terminal')]}):\n",
        "    if '__end__' not in s:\n",
        "        print(s)\n",
        "        print('--------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtZWlKBqbn5t",
        "outputId": "0778d65e-8164-48c8-caa2-59c656736083"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'supervisor': {'next': 'Coder'}}\n",
            "--------\n",
            "{'Coder': {'messages': [HumanMessage(content=\"The code `print('Hello, World!')` was executed and it printed `Hello, World!` to the terminal.\", name='Coder')]}}\n",
            "--------\n",
            "{'supervisor': {'next': 'FINISH'}}\n",
            "--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in graph.stream(\n",
        "    {'messages': [HumanMessage(content='Write a brief research report on pikas.')]},\n",
        "    {'recursion_limit': 100}):\n",
        "    if '__end__' not in s:\n",
        "        print(s)\n",
        "        print('--------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbOr10PtcJty",
        "outputId": "5baabcbc-5ffb-45cf-f567-7b2e27c2efa8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'supervisor': {'next': 'Researcher'}}\n",
            "--------\n",
            "{'Researcher': {'messages': [HumanMessage(content='# Research Report on Pikas\\n\\n## Introduction\\nPikas are small, short-legged mammals found in the mountains of western North America and much of Asia. Belonging to the genus Ochotona, they are the sole living genus of the family Ochotonidae and are closely related to rabbits and hares. Unlike their relatives, pikas lack several skeletal modifications, such as a highly arched skull and strong hind limbs.\\n\\n## Description and Behavior\\nPikas are virtually tailless and have an egg-shaped body. They are adapted to cold climates and typically live on the fringes of talus slopes and meadows with suitable vegetation. Unlike many small mammals, pikas do not hibernate and remain active throughout the winter by using tunnels under rocks and snow. They forage for grasses and other plant matter, which they stash in their dens in a process called \"haying.\" This allows them to have food supplies during the winter months.\\n\\nPikas are sensitive to high temperatures and prefer foraging in temperatures below 25 °C (77 °F). They often spend time in shaded regions to avoid direct sunlight during warmer periods. Eurasian pikas commonly live in family groups and share duties such as gathering food and keeping watch.\\n\\n## Distribution and Habitat\\nPikas inhabit a range of environments, from the isolated nunataks in Alaska and northern Canada, such as in Kluane National Park, to high altitudes on the slopes of the Himalayas. The northern pika (O. hyperborea) has a broad distribution, ranging from the Ural Mountains to the east coast of Russia and northern Japan.\\n\\nIn North America, the American pika (O. princeps) and the collared pika (O. collaris) are the two species found, with the American pika distributed throughout the mountains of western North America from central British Columbia and Alberta in Canada to various states in the United States.\\n\\n## Conservation Status\\nPikas have been used as an early warning system for detecting global warming effects due to their sensitivity to temperature changes. Recent studies suggest that populations of American pika are declining, attributed to various factors including global warming. There have been observations of pikas moving to higher elevations to find cooler temperatures and suitable habitats.\\n\\nIn 2010, the US government considered and then decided not to add the American pika under the US Endangered Species Act. However, more recent studies have found widespread extirpations and range retractions at lower elevations, which are typically warmer and drier.\\n\\n## Threats\\nHuman activity, climate change, and the reduction of forage for domestic livestock are some of the threats pikas face. Rising temperatures have been linked to reduced foraging time for pikas, limiting their ability to stockpile food for winter and potentially causing shifts in their distribution.\\n\\n## Conclusion\\nPikas are an essential part of the alpine ecosystems where they live, but they face several threats that challenge their survival. Their response to environmental changes makes them a crucial indicator species for understanding the impacts of climate change on mountainous regions.\\n\\n## References\\n- Wikipedia: [Pika](https://en.wikipedia.org/wiki/Pika), [American Pika](https://en.wikipedia.org/wiki/American_pika)\\n- Britannica: [Pika](https://www.britannica.com/animal/pika)\\n- National Park Service: [Pikas Resource Brief](https://www.nps.gov/articles/pikas-brief.htm)\\n- National Wildlife Federation: [American Pika](https://www.nwf.org/Educational-Resources/Wildlife-Guide/Mammals/American-Pika)\\n\\n(Note: This report is based on available information as of April 2023 and may not include the most recent studies or findings.)', name='Researcher')]}}\n",
            "--------\n",
            "{'supervisor': {'next': 'FINISH'}}\n",
            "--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KABBq1IctbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}