{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGzJd5ueamWumLMMo5stm8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad71/ragbot/blob/master/LangChain_QA_types.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatting with your own documents\n",
        "4 ways to do question-answering"
      ],
      "metadata": {
        "id": "kOd1d2LsfYg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain\n",
        "%pip install pypdf\n",
        "%pip install openai\n",
        "%pip install tiktoken\n",
        "%pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0BcbcxFbgU9R",
        "outputId": "95d1fc25-752b-41b3-89ec-9b323d2a973e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.20)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.6)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSvRAYdFfKk5"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader('/content/sample_data/ai_index_ch01.pdf')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV6Owh9sgTw6",
        "outputId": "6e857c4d-8671-4ea9-9c62-9751b46bfc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Chapter 1 Preview 21\\nArtificial Intelligence\\nIndex Report 2024National Affiliation\\nTo illustrate the evolving geopolitical landscape of \\nAI, the AI Index research team analyzed the country \\nof origin of notable models.\\nFigure 1.3.2 displays the total number of notable \\nmachine learning models attributed to the location \\nof researchers’ affiliated institutions.5 \\nIn 2023, the United States led with 61 notable \\nmachine learning models, followed by China with \\n15, and France with 8. For the first time since 2019, \\nthe European Union and the United Kingdom \\ntogether have surpassed China in the number of \\nnotable AI models produced (Figure 1.3.3). Since \\n2003, the United States has produced more models \\nthan other major geographic regions such as the \\nUnited Kingdom, China, and Canada (Figure 1.3.4).1.3 Frontier AI ResearchChapter 1: Research and Development Artificial Intelligence\\nIndex Report 2024\\n233444581561\\n0 5101520 25 30 35 40 45 50 55 60EgyptUnited A rab E mirat esSingapor eUnited KingdomIsraelCanadaGermanyFranc eChinaUnited S tates\\nNumber of no table machine learning modelsNumber of no table ma chine lear ning models b y\\ngeogr aphic ar ea, 20 23\\nSource: Epoch, 2023 | C hart: 202 4 AI Inde x repor t2003\\n2005\\n2007\\n2009\\n2011\\n2013\\n2015\\n2017\\n2019\\n2021\\n20230102030405060Number of no table machine learning models\\n15, China25, European U nion and\\nUnited Kingdom61, United S tatesNumber of no table ma chine lear ning models b y\\nselect geogr aphic ar ea, 20 03–23\\nSource: Epoch, 2023 | C hart: 202 4 AI Inde x repor tFigure 1.3.2\\nFigure 1.3.3\\n5 A machine learning model is considered associated with a specific country if at least one author of the paper introducing it has an affiliation with an institution based in that country. In cases \\nwhere a model’s authors come from several countries, double counting can occur.', metadata={'source': '/content/sample_data/ai_index_ch01.pdf', 'page': 20})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: load_qa_chain"
      ],
      "metadata": {
        "id": "lwggjqZ-g_0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# most generic interface\n",
        "from google.colab import userdata\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "chain = load_qa_chain(llm=OpenAI(api_key=userdata.get('OPENAI_API_KEY')), chain_type='map_reduce')\n",
        "query = 'What was used if NVIDIA A100 SXM4 was not available?'\n",
        "chain.run(input_documents=documents, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "78UyU7rdgs4w",
        "outputId": "fb79a7d0-66b0-4328-a6ae-09aab102c46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' If the NVIDIA A100 SXM4 was not available, a generalization of \"NVIDIA A100\" was used.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # it also lets you QA over a set of documents\n",
        "\n",
        "# loaders = [...]\n",
        "# documents = []\n",
        "# for loader in loaders:\n",
        "#     documents.extend(loader.load())"
      ],
      "metadata": {
        "id": "lcIgvjfZhX4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if the document is super long that it exceeds the token limit?\n",
        "\n",
        "- **Solution 1: Chain Type**\n",
        "The default `chain_type='stuff'` uses _all_ of the text from the documents in the prompt. It actually doesn't work with our example because it exceeds the token limit and causes rate-limiting errors. That's why in this example, we had to use other chain types for example `map_reduce`. Here are the other chain types:\n",
        "\n",
        "1. `map_reduce`: It separates text into batches (as an example, you can define batch size in `llm=OpenAI(batch_size=5)`, feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch.\n",
        "\n",
        "2. `refine`: It separates text into batches, feeds the first batch into LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches.\n",
        "\n",
        "3. `map_rerank`: It separates text into batches, feeds each batch to the LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.\n",
        "\n",
        "- **Solution 2: RetrievalQA**\n",
        "One issue with using ALL of the text is that it can be very costly because you are feeding all the texts to OpenAI API and the API is charged by the number of tokens. A better solution is to retrieve relevant text chunks first and only use the relevant text chunks in the language model"
      ],
      "metadata": {
        "id": "CLdmMisljAUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: RetrievalQA\n",
        "Actually uses `load_qa_chain` under the hood. We retrieve the most relevant chunks of text and feed those to the language model."
      ],
      "metadata": {
        "id": "T67sthYPlQt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "5cngqSEri6Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "retriever = db.as_retriever(search_type='similarity', search_kwargs={'k': 2})\n",
        "# `similarity` uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector\n",
        "# `mmr` uses the maximum marginal relevance search where it optimizes for similarity to query and diversity among selected documents\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(api_key=userdata.get('OPENAI_API_KEY')), chain_type='stuff', retriever=retriever, return_source_documents=True)\n",
        "query = 'How many AI publications in 2024?'\n",
        "result = qa.invoke({'query': query})"
      ],
      "metadata": {
        "id": "Pbg6f7_Zl7V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUhrRYHmmqIT",
        "outputId": "e7a90b87-03e7-4264-fb38-e591b552c6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How many AI publications in 2024?',\n",
              " 'result': \" I don't know.\",\n",
              " 'source_documents': [Document(page_content='Chapter 1 Preview 10\\nArtificial Intelligence\\nIndex Report 2024232.67\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022050100150200Number of AI p ublic ations (in tho usands)Number of AI jo urnal p ublic ations, 20 10–22\\nSource: Center for Security and E merging T echnolog y, 2023 | C hart: 202 4 AI Inde x repor tAI Journal Publications\\nFigure 1.1.6 illustrates the total number of AI journal publications from 2010 to 2022. The number of AI journal \\npublications experienced modest growth from 2010 to 2015 but grew approximately 2.4 times since 2015. \\nBetween 2021 and 2022, AI journal publications saw a 4.5% increase.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.6Artificial Intelligence\\nIndex Report 2024', metadata={'page': 9, 'source': '/content/sample_data/ai_index_ch01.pdf'}),\n",
              "  Document(page_content='Chapter 1 Preview 10\\nArtificial Intelligence\\nIndex Report 2024232.67\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022050100150200Number of AI p ublic ations (in tho usands)Number of AI jo urnal p ublic ations, 20 10–22\\nSource: Center for Security and E merging T echnolog y, 2023 | C hart: 202 4 AI Inde x repor tAI Journal Publications\\nFigure 1.1.6 illustrates the total number of AI journal publications from 2010 to 2022. The number of AI journal \\npublications experienced modest growth from 2010 to 2015 but grew approximately 2.4 times since 2015. \\nBetween 2021 and 2022, AI journal publications saw a 4.5% increase.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.6Artificial Intelligence\\nIndex Report 2024', metadata={'page': 9, 'source': '/content/sample_data/ai_index_ch01.pdf'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 3: VectorstoreIndexCreator\n",
        "VectorstoreIndexCreator is a wrapper around the above functionality. It is exactly the same under the hood, but just exposes a higher-level interface to let you get started in three lines of code."
      ],
      "metadata": {
        "id": "q63imp58p3oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "IqR5HZg1qmZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "query = 'What was used if NVIDIA A100 SXM4 was not available?'\n",
        "index.query(llm=OpenAI(api_key=userdata.get('OPENAI_API_KEY')), question=query, chain_type='stuff')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DHMDGNEUmvKu",
        "outputId": "04b1e8d5-f6fa-44c0-d8c1-23617d4709c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n\\nA generalization of the hardware type was used, such as \"NVIDIA A100.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    vectorstore_cls=Chroma\n",
        ").from_loaders([loader])\n",
        "index.query(llm=OpenAI(), question=query, chain_type='stuff')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7XCz4oxHqdX-",
        "outputId": "bbf865f2-e53e-4a60-acd0-ad5189e8b3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I do not know.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 4: ConversationalRetrievalChain\n",
        "Very similar to RetrievalQA. It added an additional parameter `chat_history` to pass in chat history which can be used for follow-up questions.\n",
        "\n",
        "ConversationalRetrievalChain = conversation memory + RetrievalQAChain\n",
        "\n",
        "If you'd like your language model to have a memory of the previous conversation, use this method.\n",
        "\n",
        "In the original example, they asked about the number of AI publications and got the result of 500,000. Then they asked the LLM to divide this number by 2. Since it has all the chat history, the model knows the number they were referring to was 500,000 and the result returned is 250,000."
      ],
      "metadata": {
        "id": "BVxnA1nIuT3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "NRh2zT4RuG_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader('/content/sample_data/ai_index_ch01.pdf')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "retriever = db.as_retriever(search_type='similarity', search_kwargs={'k': 4})\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)"
      ],
      "metadata": {
        "id": "7gXPmmZUvA7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "query = 'Which country had the most notable AI models?'\n",
        "result = qa.invoke({'question': query, 'chat_history': chat_history})"
      ],
      "metadata": {
        "id": "PyMH8dC8vkma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ThGceA4hv66U",
        "outputId": "6a8e0bd5-44d1-4af1-8cbc-4257bd44c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The United States had the most notable AI models in 2023.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result['answer'])]\n",
        "query = 'How many?'\n",
        "result = qa.invoke({'question': query, 'chat_history': chat_history})"
      ],
      "metadata": {
        "id": "tdjjae4uv7xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkQzR8j_wgs3",
        "outputId": "100a9d33-ff07-4921-8d99-a8ebafa2665e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Which country had the most notable AI models?',\n",
              "  ' The United States had the most notable AI models in 2023.')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s_8siWXhwhdP",
        "outputId": "5fbd4688-53ca-4e88-86c8-c0f6d6e07d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  The United States had 61 notable AI models in 2023.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "4 ways of QA with LangChain\n",
        "1. `load_qa_chain` uses all texts and accepts multiple documents\n",
        "2. `RetrievalQA` uses `load_qa_chain` under the hood but retrieves relevant text chunks first\n",
        "3. `VectorstoreIndexCreator` is the same as `RetrievalQA` with a higher-level interface.\n",
        "4. `ConversationalRetrievalChain` is useful when you want to pass your chat history to the model."
      ],
      "metadata": {
        "id": "hQCf0JTCxDDj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-u_uFzSwjNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}