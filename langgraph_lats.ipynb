{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Agent Tree Search\n",
    "Language Agent Tree Search (LATS) by Zhou et al. is a general LLM agent search algorithm that combines reflection/evaluation and search (specifically monte-carlo tree search) to achieve better overall task performance compared to similar techniques like ReACT, Reflexion or Tree of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import base64\n",
    "import pprint\n",
    "import asyncio\n",
    "import datetime\n",
    "import platform\n",
    "import requests\n",
    "import operator\n",
    "import playwright\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import MessageGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_executor import ToolInvocation\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.function import FunctionMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.pydantic_v1 import ValidationError\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "from langchain_core.runnables.graph import NodeColors\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "from langchain_fireworks.chat_models import ChatFireworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 4 main steps:\n",
    "1. **Select**: pick the best next actions based on the aggregate rewards from step (2). Either respond (if a solution is found or the max search depth is reached) or continue searching.\n",
    "2. **Expand and simulate**: select the \"best\" 5 potential actions and execute them in parallel.\n",
    "3. **Reflect + Evaluate**: observe the outcomes of these actions and score the decisions based on reflection (and possibly external feedback)\n",
    "4. **Backpropagate**: update the scores of the root trajectories based on the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'LATS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LATS is based on (greedy) Monte Carlo tree search. For each search step, it picks the node with the highest \"upper confidence bound\", which is a metric that balances exploitation (highest average reward) and exploration (lowest visits). Starting from that node, it generates N (5 in this case) new candidate actions to take, and adds them to the tree. It stops searching either when it has generated a valid solution OR when it has reached the maximum number of rollouts (search tree depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LangGraph state will be composed of two items:\n",
    "1. The root of the search tree\n",
    "2. The user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "The reflection chain will score agent outputs based on the decision and the tool responses. We will call this within the other two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(description='The critique and reflections on the sufficiency, superfluency, and general quality of the response')\n",
    "    score: int = Field(description='Score from 0 - 10 on the quality of the candidate response', gte=0, lte=10)\n",
    "    found_solution: bool = Field(description='Whether the response has fully solved the question or task')\n",
    "    \n",
    "    def as_message(self):\n",
    "        return HumanMessage(content=f'Reasoning: {self.reflections}\\nScore: {self.score}')\n",
    "    \n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, messages: List[BaseMessage], reflection: Reflection, parent=None):\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.messages = messages\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'<Node value={self.value}, visits={self.visits},'\n",
    "            f' solution={self.messages} reflection={self.reflection}/>'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        '''If any solutions exist, we can end the search'''\n",
    "        return self._is_solved\n",
    "    \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "    \n",
    "    @property\n",
    "    def best_child(self):\n",
    "        '''Select the child with the highest UCT to search next'''\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self._get_all_children(), key=lambda child: child.upper_confidence_bound())\n",
    "    \n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        '''Return the child with the highest value'''\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "    \n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        '''Check for how far we've rolled out the tree'''\n",
    "        if self.children:\n",
    "            return 1 + max([child.height for child in self.children])\n",
    "        return 1\n",
    "    \n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        '''Return the UCT score. This helps balance exploration vs exploitation of a branch'''\n",
    "        if self.parent is None:\n",
    "            raise ValueError('Cannot obtain UCT from root node')\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        \n",
    "        # encourages exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "\n",
    "        # encourages exploration of less-visited trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "    \n",
    "    def backpropagate(self, reward: float):\n",
    "        '''Update the score of this node and its parents'''\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def get_messages(self, include_reflections: bool=True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "        return self.messages\n",
    "    \n",
    "    def get_trajectory(self, include_reflections: bool=True) -> List[BaseMessage]:\n",
    "        '''Get messages representing this search branch'''\n",
    "        messages = []\n",
    "        node = self\n",
    "        while node:\n",
    "            messages.extend(node.get_messages(include_reflections=include_reflections)[::-1])\n",
    "            node = node.parent\n",
    "\n",
    "        # reverse the final back-tracked trajectory to return in the correct order\n",
    "        return messages[::-1] # root solution, reflection, child\n",
    "    \n",
    "    def _get_all_children(self):\n",
    "        all_nodes = []\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        return all_nodes\n",
    "    \n",
    "    def get_best_solution(self):\n",
    "        '''Return the best solution from within the current sub-tree'''\n",
    "        all_nodes = [self] + self._get_all_children()\n",
    "        best_node = max(all_nodes, key=lambda node: int(node.is_terminal and node.is_solved) * node.value)\n",
    "        return best_node\n",
    "    \n",
    "    def _mark_tree_as_solved(self):\n",
    "        parent = self.parent\n",
    "        while parent:\n",
    "            parent._is_solved = True\n",
    "            parent = parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeState(TypedDict):\n",
    "    root: Node # the full tree\n",
    "    input: str # the original input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Language Agent\n",
    "Our agent will have three primary LLM-powered processes:\n",
    "1. Reflect: score the action based on the tool response\n",
    "2. Initial response: to create the root node and start the search\n",
    "3. Expand: generate 5 candidate \"next steps\" from the best spot in the current tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more \"grounded\" tool applications (such as code synthesis), you could integrate code execution into the reflection / reward step. This type of external feedback is very useful (though adds complexity to an already complicated example notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "tools = [tavily_tool]\n",
    "tool_executor = ToolExecutor(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflection\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'Reflect and grade the assistant response to the user question below.'\n",
    "        ),\n",
    "        (\n",
    "            'user',\n",
    "            '{input}'\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='candidate')\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflection_llm_chain = (\n",
    "    prompt\n",
    "    | llm.bind_tools(tools=[Reflection], tool_choice='Reflection').with_config(run_name='Reflection')\n",
    "    | PydanticToolsParser(tools=[Reflection])\n",
    ")\n",
    "\n",
    "@as_runnable\n",
    "def reflection_chain(inputs) -> Reflection:\n",
    "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
    "    reflection = tool_choices[0]\n",
    "    if not isinstance(inputs['candidate'][-1], AIMessage):\n",
    "        reflection.found_solution = False\n",
    "    return reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Response\n",
    "We start with a single root node, generated by this first step. It responds to the user input either with a tool invocation or a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            'You are an AI assistant.'\n",
    "        ),\n",
    "        (\n",
    "            'user',\n",
    "            '{input}'\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages', optional=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "initial_answer_chain = (\n",
    "    prompt_template\n",
    "    | llm.bind_tools(tools=tools).with_config(run_name='GenerateInitialCandidate')\n",
    ")\n",
    "\n",
    "parser = JsonOutputToolsParser(return_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_iUfZAFT5pE43lTc9bXPAYRoK)\n",
      " Call ID: call_iUfZAFT5pE43lTc9bXPAYRoK\n",
      "  Args:\n",
      "    query: lithium pollution overview\n",
      "  tavily_search_results_json (call_VaI5GEq2IzD8L0hKmYD6HawT)\n",
      " Call ID: call_VaI5GEq2IzD8L0hKmYD6HawT\n",
      "  Args:\n",
      "    query: environmental impact of lithium extraction\n",
      "  tavily_search_results_json (call_odEkd2pR0Ghe7Y0dQV3kpPzr)\n",
      " Call ID: call_odEkd2pR0Ghe7Y0dQV3kpPzr\n",
      "  Args:\n",
      "    query: effects of lithium on human health\n",
      "  tavily_search_results_json (call_Z7Oibhg6DAMdJPM2TfHh2lYv)\n",
      " Call ID: call_Z7Oibhg6DAMdJPM2TfHh2lYv\n",
      "  Args:\n",
      "    query: solutions to lithium pollution\n"
     ]
    }
   ],
   "source": [
    "initial_response = initial_answer_chain.invoke({'input': 'Write a research report on lithium pollution.'})\n",
    "initial_response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Node\n",
    "We will package up the candidate generation and reflection in a single node of our graph. This is represented by the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_response(state: TreeState) -> dict:\n",
    "    '''Generate the initial candidate response'''\n",
    "    res = initial_answer_chain.invoke({'input': state['input']})\n",
    "    parsed = parser.invoke(res)\n",
    "    tool_responses = tool_executor.batch([ToolInvocation(tool=r['type'], tool_input=r['args']) for r in parsed])\n",
    "    output_messages = [res] + [ToolMessage(content=json.dumps(resp), tool_call_id=tool_call['id']) for resp, tool_call in zip(tool_responses, parsed)]\n",
    "    reflection = reflection_chain.invoke({'input': state['input'], 'candidate': output_messages})\n",
    "    root = Node(output_messages, reflection=reflection)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        'root': root\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Generation\n",
    "The following code prompts the same LLM to generate N additional candidates to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates N candidate values for a single input to sample actions from the environment\n",
    "\n",
    "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
    "    n = config['configurable'].get('N', 5)\n",
    "    bound_kwargs = llm.bind_tools(tools=tools).kwargs\n",
    "    chat_result = llm.generate(\n",
    "        [messages.to_messages()],\n",
    "        n=n,\n",
    "        callbacks=config['callbacks'],\n",
    "        run_name='GenerateCandidates',\n",
    "        **bound_kwargs\n",
    "    )\n",
    "    return [gen.message for gen in chat_result.generations[0]]\n",
    "\n",
    "expansion_chain = prompt_template | generate_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_1cxcBL5BUl5Z1lmOIe5TEBQR)\n",
      " Call ID: call_1cxcBL5BUl5Z1lmOIe5TEBQR\n",
      "  Args:\n",
      "    query: recent studies on lithium pollution 2023\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_1cxcBL5BUl5Z1lmOIe5TEBQR)\n",
      " Call ID: call_1cxcBL5BUl5Z1lmOIe5TEBQR\n",
      "  Args:\n",
      "    query: lithium pollution research 2023\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_1cxcBL5BUl5Z1lmOIe5TEBQR)\n",
      " Call ID: call_1cxcBL5BUl5Z1lmOIe5TEBQR\n",
      "  Args:\n",
      "    query: lithium pollution overview\n",
      "  tavily_search_results_json (call_YL9uAzVTE0ZqKeshlonWZRwM)\n",
      " Call ID: call_YL9uAzVTE0ZqKeshlonWZRwM\n",
      "  Args:\n",
      "    query: environmental impact of lithium mining\n",
      "  tavily_search_results_json (call_DFzaNtmwsZOPyd1WHJt5NV2A)\n",
      " Call ID: call_DFzaNtmwsZOPyd1WHJt5NV2A\n",
      "  Args:\n",
      "    query: health effects of lithium pollution\n",
      "  tavily_search_results_json (call_jdZKnabBCo8vlT73bMk6rYr0)\n",
      " Call ID: call_jdZKnabBCo8vlT73bMk6rYr0\n",
      "  Args:\n",
      "    query: lithium recycling methods\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_1cxcBL5BUl5Z1lmOIe5TEBQR)\n",
      " Call ID: call_1cxcBL5BUl5Z1lmOIe5TEBQR\n",
      "  Args:\n",
      "    query: lithium pollution research report 2023\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_1cxcBL5BUl5Z1lmOIe5TEBQR)\n",
      " Call ID: call_1cxcBL5BUl5Z1lmOIe5TEBQR\n",
      "  Args:\n",
      "    query: lithium pollution research report\n"
     ]
    }
   ],
   "source": [
    "res = expansion_chain.invoke({'input': 'Write a research report on lithium pollution.'})\n",
    "for r in res:\n",
    "    r.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate generation node\n",
    "We will package the candidate generation and reflection steps in the following 'expand' node. We do all the operations as a batch process to speed up execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
    "    '''Starting from the \"best\" node in the tree, generate N candidates for the next step'''\n",
    "    root = state['root']\n",
    "    best_candidate: Node = root.best_child if root.children else root\n",
    "    messages = best_candidate.get_trajectory()\n",
    "\n",
    "    # generate N candidates from the single child candidate\n",
    "    new_candidates = expansion_chain.invoke({'input': state['input'], 'messages': messages}, config)\n",
    "    parsed = parser.batch(new_candidates)\n",
    "\n",
    "    flattened = [\n",
    "        (i, tool_call)\n",
    "        for i, tool_calls in enumerate(parsed)\n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "\n",
    "    tool_responses = tool_executor.batch([\n",
    "        ToolInvocation(tool=tool_call['type'], tool_input=tool_call['args'])\n",
    "        for _, tool_call in flattened\n",
    "    ])\n",
    "    collected_responses = defaultdict(list)\n",
    "\n",
    "    for (i, tool_call), resp in zip(flattened, tool_responses):\n",
    "        collected_responses[i].append(ToolMessage(content=json.dumps(resp), tool_call_id=tool_call['id']))\n",
    "\n",
    "    output_messages = []\n",
    "    for i, candidate in  enumerate(new_candidates):\n",
    "        output_messages.append([candidate] + collected_responses[i])\n",
    "\n",
    "    # reflect on each candidate\n",
    "    # for tasks with external validation, you'd add that here\n",
    "    reflections = reflection_chain.batch([{'input': state['input'], 'candidate': msges} for msges in output_messages], config)\n",
    "\n",
    "    # grow tree\n",
    "    child_nodes = [\n",
    "        Node(cand, parent=best_candidate, reflection=reflection)\n",
    "        for cand, reflection in zip(output_messages, reflections)\n",
    "    ]\n",
    "\n",
    "    best_candidate.children.extend(child_nodes)\n",
    "\n",
    "    # we have already extended the tree directly, so we just return the state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph\n",
    "With those two nodes defined, we are ready to define the graph. After each agent step, we have the option of finishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_loop(state: TreeState) -> Literal['expand', '__end__']:\n",
    "    '''Determine whether to continue the tree search'''\n",
    "    root = state['root']\n",
    "    if root.is_solved:\n",
    "        return END\n",
    "    if root.height > 5:\n",
    "        return END\n",
    "    return 'expand'\n",
    "\n",
    "builder = StateGraph(TreeState)\n",
    "builder.add_node('start', generate_initial_response)\n",
    "builder.add_node('expand', expand)\n",
    "builder.set_entry_point('start')\n",
    "\n",
    "builder.add_conditional_edges('start', should_loop)\n",
    "builder.add_conditional_edges('expand', should_loop)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "----\n",
      "expand\n",
      "rolled out:  2\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "question = 'Generate a table with the average size and weight, as well as the oldest recorded instance of each of the top 5 most common birds.'\n",
    "last_step = None\n",
    "for step in graph.stream({'input': question}):\n",
    "    last_step = step\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print('rolled out: ', step_state['root'].height)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, I'll compile the relevant details to generate the table with the average size, weight, and oldest recorded instance of each of the top 5 most common birds. Here are the top 5 most common birds identified and their respective data:\n",
      "\n",
      "1. **House Sparrow**:\n",
      "   - **Average Size**: Approximately 16 cm in length.\n",
      "   - **Average Weight**: Around 24-40 grams.\n",
      "   - **Oldest Recorded Instance**: Approximately 13 years.\n",
      "\n",
      "2. **American Robin**:\n",
      "   - **Average Size**: About 25 cm in length.\n",
      "   - **Average Weight**: Roughly 77 grams.\n",
      "   - **Oldest Recorded Instance**: About 14 years.\n",
      "\n",
      "3. **European Starling**:\n",
      "   - **Average Size**: Approximately 20-23 cm in length.\n",
      "   - **Average Weight**: Around 75-90 grams.\n",
      "   - **Oldest Recorded Instance**: About 15 years.\n",
      "\n",
      "4. **Mourning Dove**:\n",
      "   - **Average Size**: About 31 cm in length.\n",
      "   - **Average Weight**: Roughly 112-170 grams.\n",
      "   - **Oldest Recorded Instance**: Approximately 31 years.\n",
      "\n",
      "5. **Northern Cardinal**:\n",
      "   - **Average Size**: About 21-23 cm in length.\n",
      "   - **Average Weight**: Around 42-48 grams.\n",
      "   - **Oldest Recorded Instance**: About 15 years.\n",
      "\n",
      "Here is the consolidated table:\n",
      "\n",
      "| Bird               | Average Size (cm) | Average Weight (grams) | Oldest Recorded Instance (years) |\n",
      "|--------------------|-------------------|------------------------|----------------------------------|\n",
      "| House Sparrow      | 16                | 24-40                  | 13                               |\n",
      "| American Robin     | 25                | 77                     | 14                               |\n",
      "| European Starling  | 20-23             | 75-90                  | 15                               |\n",
      "| Mourning Dove      | 31                | 112-170                | 31                               |\n",
      "| Northern Cardinal  | 21-23             | 42-48                  | 15                               |\n"
     ]
    }
   ],
   "source": [
    "solution_node = last_step['expand']['root'].get_best_solution()\n",
    "best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
    "print(best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "rolled out:  1\n",
      "----\n",
      "expand\n",
      "rolled out:  2\n",
      "----\n",
      "expand\n",
      "rolled out:  3\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "question = 'Write out Magnus Carlsen series of moves in his game against Alireza Firouzja and propose an alternate strategy'\n",
    "last_step = None\n",
    "for step in graph.stream({'input': question}):\n",
    "    last_step = step\n",
    "    step_name, step_state = next(iter(step.items()))\n",
    "    print(step_name)\n",
    "    print('rolled out: ', step_state['root'].height)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found a link that appears to have the complete game moves: [Carlsen vs Firouzja, 11th Norway Chess 2023](https://www.365chess.com/game.php?gid=4416560). I'll retrieve the series of moves from that game and then propose an alternate strategy.\n",
      "\n",
      "Here is the series of moves in the game:\n",
      "\n",
      "1. e4 c5\n",
      "2. Nf3 d6\n",
      "3. Bb5+ Nd7\n",
      "4. d4 cxd4\n",
      "5. Qxd4 a6\n",
      "6. Bxd7+ Bxd7\n",
      "7. O-O Nf6\n",
      "8. Nc3 e6\n",
      "9. Rd1 Be7\n",
      "10. Bg5 Bc6\n",
      "11. Qd3 O-O\n",
      "12. Nd4 Rc8\n",
      "13. a4 Qb6\n",
      "14. a5 Qc5\n",
      "15. Be3 Qh5\n",
      "16. f3 Nd7\n",
      "17. Nxc6 Rxc6\n",
      "18. Ne2 Rfc8\n",
      "19. c3 Ne5\n",
      "20. Qb1 Nc4\n",
      "21. Bf2 Qh6\n",
      "22. Nd4 R6c7\n",
      "23. Qc2 d5\n",
      "24. exd5 exd5\n",
      "25. Re1 Bf8\n",
      "26. Re2 Qg6\n",
      "27. Qxg6 hxg6\n",
      "28. Nb3 Re7\n",
      "29. Rc1 Rce8\n",
      "30. Rxe7 Rxe7\n",
      "31. Kf1 Re6\n",
      "32. Rd1 Rd6\n",
      "33. Re1 Rd8\n",
      "34. Re2 f6\n",
      "35. Ke1 Kf7\n",
      "36. Kd1 Ne5\n",
      "37. Kc2 Nc4\n",
      "38. Kc1 Bd6\n",
      "39. h3 Bc7\n",
      "40. Kc2 Bxa5\n",
      "41. Nb3 Bb6\n",
      "42. Nd4 Rd7\n",
      "43. Kb3 Ne5\n",
      "44. Kc2 Nc4\n",
      "45. Kd3 Re7\n",
      "46. Rc2 Ne5+\n",
      "47. Kd2 Nc4+\n",
      "48. Kd3 Ne5+\n",
      "49. Kd2 Rd7\n",
      "50. Ke2 Bc7\n",
      "51. Nb3 Bd6\n",
      "52. Nd2 Re7\n",
      "53. Kf1 Nd3\n",
      "54. Nb3 Bf4\n",
      "55. g3 Bh6\n",
      "56. Nc5 Nxc5\n",
      "57. Bxc5 Re6\n",
      "58. h4 g5\n",
      "59. hxg5 Bxg5\n",
      "60. Bd4 Be3\n",
      "61. Bxe3 Rxe3\n",
      "62. Kf2 Re7\n",
      "63. Rd2 Ke6\n",
      "64. Rd4 b5\n",
      "65. g4 g5\n",
      "66. Rd1 a5\n",
      "67. Ra1 a4\n",
      "68. Rd1 Rd7\n",
      "69. Ke3 Ke5\n",
      "70. Rd4 Rc7\n",
      "71. Rb4 Rc5\n",
      "72. Kd3 Kd6\n",
      "73. Rd4 Rc8\n",
      "74. Rb4 Kc5\n",
      "75. Rd4 Rh8\n",
      "76. Kc2 Rh2+\n",
      "77. Kb1 Rf2\n",
      "78. Rd3 Kc4\n",
      "79. Re3 Kb3\n",
      "80. c4+ Kxc4\n",
      "81. Re6 Rxf3\n",
      "82. Rc6+ Kb4\n",
      "83. Rc7 Rf4\n",
      "84. Rxg7 d4\n",
      "85. Kc2 d3+\n",
      "86. Kxd3 Kb3\n",
      "87. Rb7 b4\n",
      "88. Kd2 Rxg4\n",
      "89. Kd3 Rg3+\n",
      "90. Kd2 Rg2+\n",
      "91. Kd3 Rxb2\n",
      "92. Rb6 Rc2\n",
      "93. Rb7 Rc6\n",
      "94. Kd2 Kc4\n",
      "95. Rd7 Rc5\n",
      "96. Kd1 Rd5+\n",
      "97. Rxd5 Kxd5\n",
      "98. Kd2 Kc4\n",
      "99. Kc2 b3+\n",
      "100. Kb2 Kb4\n",
      "101. Kb1 Kc3\n",
      "102. Kc1 b2+\n",
      "103. Kb1 Kb3\n",
      "104. Ka1 g4\n",
      "105. Kb1 f5\n",
      "106. Ka1 f4\n",
      "107. Kb1 g3\n",
      "108. Ka1 g2\n",
      "109. Kb1 g1=Q#\n",
      "\n",
      "Magnus Carlsen won the game with the white pieces.\n",
      "\n",
      "### Alternate Strategy Proposal\n",
      "\n",
      "One possible alternate strategy for Alireza Firouzja could have been to apply more pressure in the early middle game, particularly focusing on controlling the center and creating dynamic play. Here are a few alternate moves that might have changed the game's flow:\n",
      "\n",
      "1. After 8. Nc3, instead of 8...e6, consider 8...g6 to fianchetto the bishop, aiming for a more dynamic position.\n",
      "2. On move 11, instead of 11...O-O, consider 11...Qb6 to increase pressure on the queenside and centralize the queen.\n",
      "3. On move 15, instead of 15...Qh5, consider 15...Qe7 to keep the queen more centralized and flexible.\n",
      "4. After 17. Nxc6, instead of 17...Rxc6, consider 17...Bxc6 to maintain better pawn structure and open up the long diagonal for the bishop.\n",
      "5. On move 21, instead of 21...Qh6, consider 21...Nd7 to reposition the knight and prepare for potential central breakthroughs.\n",
      "\n",
      "By applying these adjustments, Firouzja could have aimed for a more balanced and dynamic position, potentially avoiding the pressure Carlsen was able to build up in the game.\n"
     ]
    }
   ],
   "source": [
    "solution_node = last_step['expand']['root'].get_best_solution()\n",
    "best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
    "print(best_trajectory[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "LATS is reasonably fast and effective at solving complex reasoning tasks. A few notes that we've observed above:\n",
    "1. While effective, the tree rollout can take additional compute time. If you wanted to include this in a production app, you'd either want to ensure that intermediate steps are streamed (so that the user sees the thinking process / has access to intermediate results) or use it for fine-tuning data to improve the single-shot accuracy and avoid long rollouts.\n",
    "2. The candidate selection process is only as good as the reward you generate. Here we are using self-reflection exclusively, but if you have an external source of feedback (such as code test execution), that should be incorporated in the locations mentioned above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinegap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
