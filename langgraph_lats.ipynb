{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Agent Tree Search\n",
    "Language Agent Tree Search (LATS) by Zhou et al. is a general LLM agent search algorithm that combines reflection/evaluation and search (specifically monte-carlo tree search) to achieve better overall task performance compared to similar techniques like ReACT, Reflexion or Tree of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import base64\n",
    "import asyncio\n",
    "import datetime\n",
    "import platform\n",
    "import requests\n",
    "import operator\n",
    "import playwright\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Annotated\n",
    "from typing import TypedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import MessageGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_executor import ToolInvocation\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.chat import ChatMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.function import FunctionMessage\n",
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.pydantic_v1 import ValidationError\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "from langchain_core.runnables.graph import NodeColors\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "from langchain_fireworks.chat_models import ChatFireworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 4 main steps:\n",
    "1. **Select**: pick the best next actions based on the aggregate rewards from step (2). Either respond (if a solution is found or the max search depth is reached) or continue searching.\n",
    "2. **Expand and simulate**: select the \"best\" 5 potential actions and execute them in parallel.\n",
    "3. **Reflect + Evaluate**: observe the outcomes of these actions and score the decisions based on reflection (and possibly external feedback)\n",
    "4. **Backpropagate**: update the scores of the root trajectories based on the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'LATS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LATS is based on (greedy) Monte Carlo tree search. For each search step, it picks the node with the highest \"upper confidence bound\", which is a metric that balances exploitation (highest average reward) and exploration (lowest visits). Starting from that node, it generates N (5 in this case) new candidate actions to take, and adds them to the tree. It stops searching either when it has generated a valid solution OR when it has reached the maximum number of rollouts (search tree depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LangGraph state will be composed of two items:\n",
    "1. The root of the search tree\n",
    "2. The user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(description='The critique and reflections on the sufficiency, superfluency, and general quality of the response')\n",
    "    score: int = Field(description='Score from 0 - 10 on the quality of the candidate response', gte=0, lte=10)\n",
    "    found_solution: bool = Field(description='Whether the response has fully solved the question or task')\n",
    "    \n",
    "    def as_message(self):\n",
    "        return HumanMessage(content=f'Reasoning: {self.reflections}\\nScore: {self.score}')\n",
    "    \n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, messages: List[BaseMessage], reflection: Reflection, parent: Optional[Node]=None):\n",
    "        self.value = 0\n",
    "        self.visits = 0\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.messages = messages\n",
    "        self.reflection = reflection\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        if self._is_solved:\n",
    "            self._mark_tree_as_solved()\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f'<Node value={self.value}, visits={self.visits},'\n",
    "            f' solution={self.messages} reflection={self.reflection}/>'\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def is_solved(self):\n",
    "        '''If any solutions exist, we can end the search'''\n",
    "        return self._is_solved\n",
    "    \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return not self.children\n",
    "    \n",
    "    @property\n",
    "    def best_child(self):\n",
    "        '''Select the child with the highest UCT to search next'''\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self._get_all_children(), key=lambda child: child.upper_confidence_bound())\n",
    "    \n",
    "    @property\n",
    "    def best_child_score(self):\n",
    "        '''Return the child with the highest value'''\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
    "    \n",
    "    @property\n",
    "    def height(self) -> int:\n",
    "        '''Check for how far we've rolled out the tree'''\n",
    "        if self.children:\n",
    "            return 1 + max([child.height for child in self.children])\n",
    "        return 1\n",
    "    \n",
    "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
    "        '''Return the UCT score. This helps balance exploration vs exploitation of a branch'''\n",
    "        if self.parent is None:\n",
    "            raise ValueError('Cannot obtain UCT from root node')\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        \n",
    "        # encourages exploitation of high-value trajectories\n",
    "        average_reward = self.value / self.visits\n",
    "\n",
    "        # encourages exploration of less-visited trajectories\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return average_reward + exploration_weight * exploration_term\n",
    "    \n",
    "    def backpropagate(self, reward: float):\n",
    "        '''Update the score of this node and its parents'''\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def get_messages(self, include_reflections: bool=True):\n",
    "        if include_reflections:\n",
    "            return self.messages + [self.reflection.as_message()]\n",
    "        return self.messages\n",
    "    \n",
    "    def get_trajectory(self, include_reflections: bool=True) -> List[BaseMessage]:\n",
    "        '''Get messages representing this search branch'''\n",
    "        messages = []\n",
    "        node = self\n",
    "        while node:\n",
    "            messages.extend(node.get_messages(include_reflections=include_reflections)[::-1])\n",
    "            node = node.parent\n",
    "\n",
    "        # reverse the final back-tracked trajectory to return in the correct order\n",
    "        return messages[::-1] # root solution, reflection, child\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinegap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
